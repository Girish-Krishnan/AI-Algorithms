{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 Lab 5 Part 1: Taxi Routing with Dynamic Programming\n",
    "\n",
    "In this part of the lab, you will apply dynamic programming techniques to solve a taxi routing problem in a city grid using the Gymnasium \"Taxi-v3\" environment. You will implement Policy Iteration and Value Iteration algorithms and analyze their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi-v3 Environment Details\n",
    "\n",
    "Please read the following documentation to learn more about the Taxi-v3 environment: [https://gymnasium.farama.org/environments/toy_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "Make sure you understand the state space, action space, and reward structure of the environment before proceeding.\n",
    "\n",
    "## Problem Description\n",
    "Your task is to optimize taxi movements to efficiently pick up and drop off passengers. The environment is a simplified city grid where the taxi must navigate to pick up and drop off passengers at designated locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Taxi environment using Gymnasium\n",
    "env = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n",
    "\n",
    "# Print environment details\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Programming Algorithms for MDPs\n",
    "\n",
    "Here is a brief overview of the dynamic programming algorithms you will implement:\n",
    "\n",
    "#### Policy Iteration\n",
    "1. **Initialize:** Initialize an arbitrary policy $\\pi(s)$ and the value function $V(s) = 0$ for all states.\n",
    "2. **Policy Evaluation:** For a given policy, solve:\n",
    "   $$V^{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^{\\pi}(s')]$$\n",
    "3. **Policy Improvement:** Update the policy:\n",
    "   $$\\pi(s) \\leftarrow \\arg \\max_{a} \\sum_{s', r} p(s', r | s, a) [r + \\gamma V^{\\pi}(s')]$$\n",
    "4. **Repeat** steps 2-3 until the policy stabilizes.\n",
    "\n",
    "#### Value Iteration\n",
    "1. **Initialize:** Initialize the value function $V(s) = 0$ for all states.\n",
    "2. **Update:** For each state $s$:\n",
    "   $$V(s) \\leftarrow \\max_{a} \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]$$\n",
    "3. **Repeat** step 2 until the value function stabilizes.\n",
    "4. **Policy Extraction:** Extract the policy:\n",
    "   $$\\pi(s) = \\arg \\max_{a} \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below to implement Policy Iteration algorithm for the Taxi-v3 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.9, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform policy iteration to find the optimal policy and value function for a given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object representing the problem.\n",
    "    - gamma: The discount factor for future rewards (default: 0.9).\n",
    "    - max_iterations: The maximum number of iterations to perform (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - policy: The optimal policy, represented as an array of actions for each state.\n",
    "    - value_function: The value function corresponding to the optimal policy.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize the number of states and actions\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the policy and value function\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    value_function = np.zeros(n_states)\n",
    "\n",
    "    # Function to calculate the action-value function using one-step lookahead\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Compute action-value function using one-step lookahead.\n",
    "\n",
    "        Args:\n",
    "            state (int): Current state.\n",
    "            V (np.ndarray): Current value function.\n",
    "\n",
    "        Returns:\n",
    "            A (np.ndarray): Action-value array.\n",
    "        \"\"\"\n",
    "        A = np.zeros(n_actions)\n",
    "        for action in range(n_actions):\n",
    "            for prob, next_state, reward, _ in env.P[state][action]:\n",
    "                # TODO: Calculate the expected value of each action\n",
    "                # The reward is the immediate reward plus the discounted future reward\n",
    "                ...\n",
    "        return A\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        stable_policy = True\n",
    "        for state in range(n_states):\n",
    "            # TODO: Retrieve the current action from the policy\n",
    "            \n",
    "\n",
    "            # TODO: Compute the action-values using one-step lookahead\n",
    "            \n",
    "\n",
    "            # TODO: Select the best action (the one with the highest expected value)\n",
    "            \n",
    "\n",
    "            # TODO: Check if the current policy action is different from the best action\n",
    "            # If so, set the stable_policy flag to False\n",
    "            \n",
    "            # TODO: Update the policy at the current state with the best action\n",
    "\n",
    "        # Policy Evaluation\n",
    "        for state in range(n_states):\n",
    "            # TODO: Update the value function for each state based on the current policy\n",
    "            ...\n",
    "\n",
    "        # TODO: If the policy is stable, break out of the loop\n",
    "        ...\n",
    "\n",
    "    return policy, value_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below to implement Value Iteration algorithm for the Taxi-v3 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, max_iterations=1000, threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Perform value iteration to find the optimal policy and value function for a given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object representing the problem.\n",
    "    - gamma: The discount factor for future rewards (default: 0.9).\n",
    "    - max_iterations: The maximum number of iterations to perform (default: 1000).\n",
    "    - threshold: The convergence threshold for the value function.\n",
    "\n",
    "    Returns:\n",
    "    - policy: The optimal policy, represented as an array of actions for each state.\n",
    "    - value_function: The value function corresponding to the optimal policy.\n",
    "    \"\"\"\n",
    "    # Initialize the number of states and actions\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the value function\n",
    "    value_function = np.zeros(n_states)\n",
    "\n",
    "    # Function to calculate the action-value function using one-step lookahead\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Compute action-value function using one-step lookahead.\n",
    "\n",
    "        Args:\n",
    "            state (int): Current state.\n",
    "            V (np.ndarray): Current value function.\n",
    "\n",
    "        Returns:\n",
    "            A (np.ndarray): Action-value array.\n",
    "        \"\"\"\n",
    "        A = np.zeros(n_actions)\n",
    "        for action in range(n_actions):\n",
    "            # TODO: Fill in the transition dynamics (probability, next state, reward, done)\n",
    "            for prob, next_state, reward, _ in env.P[state][action]:\n",
    "                # TODO: Calculate the expected value of each action\n",
    "                # Similar to policy iteration, the reward is the immediate reward plus the discounted future reward\n",
    "                ...\n",
    "        return A\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        for state in range(n_states):\n",
    "            # TODO: Compute the action-values using one-step lookahead\n",
    "\n",
    "            # TODO: Update the value function for the state\n",
    "\n",
    "            # TODO: Calculate the maximum change (delta) between the current value and the new values\n",
    "\n",
    "        # TODO: Check for convergence: i.e., if the change in value is less than the threshold, break out of the loop\n",
    "\n",
    "    # Extract the policy based on the optimal value function\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    for state in range(n_states):\n",
    "        # TODO: Determine the best action for each state\n",
    "        ...\n",
    "\n",
    "    return policy, value_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below evaluates the policy in the environment and returns the total reward for that policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a given policy\n",
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to evaluate both the Policy Iteration and Value Iteration algorithms on the Taxi-v3 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Policy Iteration\n",
    "policy_pi, value_pi = policy_iteration(env)\n",
    "print(\"Policy Iteration:\")\n",
    "print(\"Average reward:\", evaluate_policy(env, policy_pi))\n",
    "\n",
    "# Run Value Iteration\n",
    "policy_vi, value_vi = value_iteration(env)\n",
    "print(\"\\nValue Iteration:\")\n",
    "print(\"Average reward:\", evaluate_policy(env, policy_vi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we generate an animation of the taxi navigating the environment using the optimal policy found by the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_animation(env, policy, filename='taxi_animation.gif', seed=42):\n",
    "    frames = []\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = policy[state]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimsave(filename, frames, fps=5)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the animations for both policies\n",
    "animation_pi = generate_animation(env, policy_pi, 'policy_iteration.gif')\n",
    "animation_vi = generate_animation(env, policy_vi, 'value_iteration.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the gifs generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=animation_pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=animation_vi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "Please submit the completed notebook (after running all cells) as a .ipynb file to Gradescope, along with the two .gif files generated at the end of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

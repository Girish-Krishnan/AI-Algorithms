{"cells":[{"cell_type":"markdown","metadata":{"id":"HTB_8TQI54V9"},"source":["# COGS 188 Lab 4 Part 1: Bandits\n","\n","In this part, you will explore a few different strategies for solving the multi-armed bandit problem, a classic example used in reinforcement learning. The \"multi-armed bandit\" is a scenario where you must choose between multiple actions (the arms of the bandit), each with a different and unknown reward probability distribution. The goal is to maximize your rewards over time.\n","\n","The multi-armed bandit problem is named after a slot machine (or \"one-armed bandit\") but with multiple levers. Each lever has a different probability of paying out a reward, and you want to discover which lever offers the best payout rate with the least number of trials.\n","\n","## Mathematical Framework\n","\n","### Problem Setting\n","Consider a scenario where you have a set of $K$ slot machines (arms of the bandit). Each machine provides a reward drawn from a probability distribution that is specific to that machine but unknown to the agent. When the agent selects an arm $i$, it receives a reward $R_i$ based on the arm's reward distribution.\n","\n","### Key Concepts:\n","- **Action ($a$)**: Pulling a lever of a specific slot machine.\n","- **Reward ($R_a$)**: The payout received after taking action $a$.\n","- **Value of an Action ($q_*(a)$)**: The expected reward from action $a$, given by $q_*(a) = \\mathbb{E}[R_a]$.\n","\n","### Objective:\n","The goal is to maximize the sum of rewards over some time period, typically by learning to choose the best machine through trial and experience.\n","\n","## Strategies for Solving Bandit Problems\n","\n","### Greedy Algorithm\n","The greedy algorithm always chooses the action that has the highest estimated reward based on the information available up to that point.\n","\n","#### Action Selection:\n","$$A_t = \\underset{a}{\\mathrm{argmax}} \\, Q_t(a)$$\n","where $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n","\n","### Epsilon-Greedy Algorithm\n","This algorithm introduces a small probability $\\epsilon$ that the agent will explore the environment rather than exploit it. This probability ensures that every action is sampled sufficiently often.\n","\n","#### Action Selection:\n","$$A_t = \\begin{cases}\n","a^* = \\underset{a}{\\mathrm{argmax}} \\, Q_t(a) & \\text{with probability } 1 - \\epsilon \\\\\n","\\text{Random action from } \\{1, \\dots a^*-1, a^*+1, \\dots  K\\} & \\text{ with probability } \\epsilon\n","\\end{cases}$$\n","\n","$\\epsilon$ is the probability of exploration, which involves randomly choosing any other action **except** the greedy action.\n","\n","With $\\epsilon = 0$, the epsilon-greedy algorithm is equivalent to the greedy algorithm.\n","\n","### Update Rule:\n","Regardless of the strategy (greedy or epsilon-greedy), after selecting an action $A_t$ and observing a reward $R_t$, the estimated value $Q_t(A_t)$ is updated as follows:\n","\n","#### Incremental Update Formula:\n","$$Q_{t+1}(A_t) = Q_t(A_t) + \\alpha_t (R_t - Q_t(A_t))$$\n","where $\\alpha_t$ is the learning rate, which might typically be set as $\\alpha_t = \\frac{1}{\\text{count}(A_t)}$, the reciprocal of the number of times action $A_t$ has been selected. This update step adjusts the action-value estimate towards the actual reward received, refining the estimate based on the latest information.\n","\n","## Applications of Bandit Algorithms\n","Bandit algorithms have practical applications in various fields including:\n","\n","- **Internet Advertising**: Optimizing ad selections to maximize click-through rates.\n","- **Clinical Trials**: Efficiently determining the most effective treatment options.\n","- **Finance**: Portfolio optimization under uncertainty.\n","- **Recommendation Systems**: Dynamically adjusting recommendations to align with user preferences.\n","\n","Understanding the theory and implementation of multi-armed bandits allows for the development of efficient decision-making processes that are critical in many areas of business and science. In this part of the lab, you will implement and compare the performance of the greedy and epsilon-greedy algorithms in solving the multi-armed bandit problem.\n","\n","## Setup and Initial Exploration\n","\n","### Libraries and Initial Setup\n","\n","Import the necessary libraries and configure the environment for visualization. Make sure you have `numpy`, `matplotlib`, `seaborn`, and `pandas` installed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9aq5yGb54WF"},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"cZMIG7z354WH"},"source":["### Bandit Setup\n","\n","We begin by defining the number of arms, trials, and plays. The number of arms represents the number of slot machines in the bandit problem. The number of trials is the total number of times the agent will select an arm, and the number of plays is the number of times each arm will be played in a trial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qrwAUoH54WH"},"outputs":[],"source":["n_arms = 10\n","n_trials = 2000\n","n_plays = 2000"]},{"cell_type":"markdown","metadata":{"id":"bqAF9Vrd54WI"},"source":["## Task 1: Implement Core Functions\n","\n","### Update Function\n","Implement the `update` function to adjust the value estimates based on received rewards. For the learning rate $\\alpha_t$, use the formula $\\alpha_t = \\frac{1}{\\text{count}(A_t)}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhxfmF1S54WI"},"outputs":[],"source":["def update(q, r, k):\n","    \"\"\"\n","    Update the Q-value using the given reward and number of times the action has been taken.\n","\n","    Parameters:\n","    q (float): The current Q-value.\n","    r (float): The reward received for the action.\n","    k (int): The number of times the action has been taken before. Note this is NOT the time index.\n","\n","    Returns:\n","    float: The updated Q-value.\n","    \"\"\"\n","    # Note: since k is the number of times the action has been taken before this update, we need to add 1 to k before using it in the formula.\n","\n","    # TODO: complete the formula to update the Q-value"]},{"cell_type":"markdown","metadata":{"id":"sYiZGfGQ54WJ"},"source":["### Greedy Action Selection\n","\n","Create a function to select the action with the highest estimated value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6L63AzA54WJ"},"outputs":[],"source":["def greedy(q_estimate):\n","    \"\"\"\n","    Selects the action with the highest Q-value.\n","\n","    Parameters:\n","    q_estimate (numpy.ndarray): 1-D Array of Q-values for each action.\n","\n","    Returns:\n","    int: The index of the action with the highest Q-value.\n","    \"\"\"\n","    # TODO: return the index of the action with the highest Q-value"]},{"cell_type":"markdown","metadata":{"id":"gZ-z6KF154WK"},"source":["### Epsilon-Greedy Action Selection\n","\n","Actually we will NEVER use the greedy function you just wrote :). But you can modify the existing greedy function to include an exploration factor, $\\epsilon$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bu5hXbNv54WK"},"outputs":[],"source":["def egreedy(q_estimate, epsilon):\n","    \"\"\"\n","    Implements the epsilon-greedy exploration strategy for multi-armed bandits.\n","\n","    Parameters:\n","    q_estimate (numpy.ndarray): 1-D Array of estimated action values.\n","    epsilon (float): Exploration rate, determines the probability of selecting a random action.\n","\n","    Returns:\n","    int: The index of the selected action.\n","    \"\"\"\n","    # TODO: implement the epsilon-greedy strategy"]},{"cell_type":"markdown","metadata":{"id":"wHtyI2Ta54WL"},"source":["## Simulating Bandit Strategies\n","\n","### Simulation Setup\n","\n","In this task, we will simulate the maximum value from bandits over multiple trials. The bandits represent the different slot machines in the multi-armed bandit problem. Each bandit has a reward distribution that is specific to that machine but unknown to the agent. The goal is to determine the bandit with the highest payout rate through trial and experience.\n","\n","To simulate the bandit strategies, we will first generate a set of bandits with rewards drawn from a normal distribution, and find the maximum value from the bandits over multiple trials. The reward for each arm is drawn from a normal distribution with a mean of 0 and a standard deviation of 1, using the `np.random.normal` function.\n","\n","For each trial, we can find the maximum value from the bandits by selecting the arm with the highest estimated value.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6Sj6Jvg54WM"},"outputs":[],"source":["maxv = np.empty(10000)\n","for idx in range(10000):\n","    bandits = np.random.normal(size=n_arms)\n","    maxv[idx] = np.max(bandits)"]},{"cell_type":"markdown","metadata":{"id":"FVF6fiC354WN"},"source":["### Visualization of Maximum Values\n","The cell below visualizes the distribution of maximum $Q$-values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udFlOa_b54WP"},"outputs":[],"source":["sns.histplot(maxv)\n","mave = np.mean(maxv)\n","plt.title(f'Max Q of 10 armed bandit over 10k trials: mean={mave:.3f}')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Zu_Zv6PZ54WP"},"source":["## Task 2: Experiment and Analysis\n","\n","### Running the Experiments\n","\n","Now, we will run experiments using different strategies to solve the multi-armed bandit problem. We will compare the performance of the greedy and epsilon-greedy algorithms by plotting the average rewards over time.\n","\n","Similar to the previous task, the reward for each arm is drawn from a normal distribution with a mean of 0 and a standard deviation of 1, via the `np.random.normal` function.\n","\n","Your task is to complete the code below by calling the `egreedy` function that you defined earlier, and perform experiments with **three** different epsilon values for the epsilon-greedy strategy. Your list of epsilon values should include 0 (greedy strategy) and two other values of your choice that are in the range $\\epsilon \\in (0, 0.3]$.\n","\n","You can expect this cell to take at least 1 minute to run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPpCCgU854WQ"},"outputs":[],"source":["data = []\n","epsilon_values = ... # TODO: Fill in the list of epsilon values to test\n","for epsilon in epsilon_values:\n","    rewards = [] # stores the rewards for each trial\n","\n","    for trial in range(n_trials):\n","        # TODO: Use the egreedy and update functions to implement the epsilon-greedy strategy\n","        # Initialize the Q-values, rewards, and number of times each action has been taken\n","        # Remember that there are n_arms number of actions, and the Q-values, rewards, and counts should be stored for each action\n","        # Also, there are n_plays number of plays in each trial\n","        bandits = np.random.normal(size=n_arms) # the true values of each bandit\n","        q_estimate = np.zeros(n_arms) # the estimated values of each bandit\n","        k = np.zeros(n_arms) # the number of times each bandit has been played\n","        rs = np.empty(n_plays) # stores the rewards for each play\n","\n","        for play in range(n_plays):\n","            action = ... # TODO: Select an action\n","            r = ... # TODO: Get the reward for the selected action, using a normal distribution with mean=bandits[action] and std=1\n","            rs[play] = r\n","            # TODO: Update the Q-value for the selected action\n","            q_estimate[action] = ...\n","\n","            # TODO: Update the number of times the action has been taken\n","            ...\n","\n","        rewards.append(rs)\n","\n","    df = pd.DataFrame(rewards).reset_index().melt(id_vars=['index'])\n","    df.rename(columns={'index':'trials','variable':'plays', 'value':'reward'}, inplace=True)\n","    if epsilon==0:\n","        df['epsilon']='greedy'\n","    else:\n","        df['epsilon']=f'\\u03F5={epsilon}'\n","    data.append(df)\n","\n","\n","alldata = pd.concat(data)"]},{"cell_type":"markdown","metadata":{"id":"hDFPwrXH54WR"},"source":["### Visualization of Strategy Performance\n","\n","We can visualize and compare the performance of different strategies by running the cell below. This might take a little more than 2 minutes to run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYiHMjhF54WS"},"outputs":[],"source":["sns.set_style('white')\n","sns.lineplot(data=alldata, x='plays', y='reward', hue='epsilon', palette='Set2')\n","plt.axhline(mave, label='Mean max value')\n","plt.legend()\n","plt.title('Comparison of Bandit Strategies Over Time')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"60nzSjcd54WS"},"source":["### Early Performance Analysis\n","\n","We can analyze the early game performance using line plots for smaller number of plays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StOfvHJj54WT"},"outputs":[],"source":["sns.lineplot(data=alldata.query('plays < 100'), x='plays', y='reward', hue='epsilon', palette='Set2')\n","plt.title('Early Game Analysis of Bandit Strategies')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FzVf9n1G54WT"},"source":["## Task 3: Compare Optimistic Initial Conditions with Standard Initialization\n","\n","### Background on Optimistic Initial Conditions\n","\n","Optimistic Initial Conditions (IC) involve setting the initial estimates of the action values much higher than expected. This encourages exploration early in the learning process, as the agent is more likely to try all actions before settling on an apparently optimal one due to high initial rewards being corrected.\n","\n","### Experiment Setup\n","\n","Using the same set up as before, your task is to run simulations using both optimistic and non-optimistic initial conditions to observe how they affect the convergence speed and overall performance.\n","\n","For the non-optimistic initial conditions, set the initial action values to 0 for each action. For the optimistic initial conditions, set the initial action values to 5 for each action."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFpBwv9d54WU"},"outputs":[],"source":["data_optimistic = []\n","data_standard = []\n","\n","# TODO: Following a similar structure as before, run experiments for:\n","# - Optimistic initial values: q_estimate = np.ones(n_arms) * 5\n","# - Realistic initial values: q_estimate = np.zeros(n_arms)\n","# For both experiments, use three different values of epsilon: 0, 0.1, and 0.01\n","\n","\n","\n","# Following similar code structure as before, the data will be stored in alldata_optimistic and alldata_standard, which are Pandas DataFrames\n","\n","alldata_standard = pd.concat(data_standard)\n","alldata_optimistic = pd.concat(data_optimistic)"]},{"cell_type":"markdown","metadata":{"id":"__BavzWO54WU"},"source":["Once again, we can plot the average rewards over time to compare the performance of the two strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iDYQGVc54WV"},"outputs":[],"source":["fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n","\n","# Plot for Standard Initialization\n","sns.lineplot(data=alldata_standard, x='plays', y='reward', hue='epsilon', style='epsilon', markers=True, dashes=False, palette='Set1', ax=axes[0])\n","axes[0].set_title('Standard Initialization Performance')\n","axes[0].set_xlabel('Plays')\n","axes[0].set_ylabel('Average Reward')\n","axes[0].legend(title='Strategy and Initialization')\n","\n","# Plot for Optimistic Initialization\n","sns.lineplot(data=alldata_optimistic, x='plays', y='reward', hue='epsilon', style='epsilon', markers=True, dashes=False, palette='Set2', ax=axes[1])\n","axes[1].set_title('Optimistic Initialization Performance')\n","axes[1].set_xlabel('Plays')\n","axes[1].set_ylabel('Average Reward')\n","axes[1].legend(title='Strategy and Initialization')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cSwYwAZN54WV"},"source":["## Optional, Ungraded Exercise: Compare the Performance of Different Strategies\n","\n","How do the greedy and epsilon-greedy strategies compare in terms of performance and convergence speed? What are the trade-offs between exploration and exploitation in these strategies? Furthermore, do optimistic initial conditions improve the performance?\n","\n","Discuss the implications of these trade-offs in real-world applications."]},{"cell_type":"markdown","metadata":{"id":"NEzNvhF754WV"},"source":["## Task 4: Visualizing Bandit Strategies for a Slot Machine Simulator\n","\n","The next part of this assignment involves using `slot_machine.py`. You will work with a simple slot machine simulator that models a multi-armed bandit problem using the $\\epsilon$-greedy algorithm. The goal is to maximize your rewards by selecting slot machines based on past rewards and an exploration strategy that balances the trade-off between exploiting the best-known machine and exploring other machines.\n","\n","**Note**: for this part, you will need to run the code in a local environment with a graphical interface (e.g., your local machine), similar to how you did L2 and L3. The code will not run in the notebook environment.\n","\n","### Problem Statement\n","You are faced with `n` slot machines, each with a different probability of paying out a reward. These probabilities are unknown to you at the start. You can play one machine per round, and each play has a cost. Your objective is to maximize your net rewards over a series of plays.\n","\n","### Environment Setup\n","The simulator is set up using Pygame, a popular library for writing video games in Python. The environment consists of:\n","\n","- **n_machines**: Number of slot machines.\n","- **true_rewards**: Array storing the true probability of each machine giving a reward.\n","- **estimated_rewards**: Array to maintain our estimates of each machine's reward probabilities.\n","- **play_counts**: Array tracking how many times each machine has been played.\n","- **cost_per_play**: Cost incurred each time a machine is played.\n","- **total_reward**: Net reward accumulated over all plays.\n","- **epsilon**: Probability of choosing to explore (i.e., selecting a machine at random).\n","\n","### Code Structure\n","The code is organized as follows:\n","\n","- **Initialization**: Set up the Pygame environment, define screen dimensions, colors, fonts, and initialize game settings.\n","- **Game Loop**:\n","  - **Event Handling**: Process mouse clicks for selecting machines in manual mode.\n","  - **AI Mode Logic**: Implement the epsilon-greedy algorithm to select which machine to play based on the estimated rewards and exploration factor.\n","  - **Reward Calculation**: Update the total rewards based on the machine's payout and the cost per play.\n","  - **Display Updates**: Render the slot machines, number of plays, and total rewards on the screen.\n","\n","## Tasks to Complete\n","To complete this assignment, you need to fill in the missing parts of the code that are marked with `# TODO`. The main tasks are as follows:\n","\n","1. **Update Estimated Rewards**: After playing a machine, update your estimate of its reward probability based on the outcome.\n","2. **Implement Epsilon-Greedy Selection**:Randomly decide (based on epsilon) whether to explore (choose a machine randomly) or exploit (choose the best-known machine).\n","3. **Simulate Reward Collection:** Simulate obtaining a reward from the chosen machine based on its true reward probability.\n","4. **Update Play Counts and Total Reward:** Update the count of plays for the selected machine and adjust the total reward based on whether the machine paid out.\n","5. **Incremental Update Formula:** Use the incremental update formula to update the estimated reward probability after each play.\n","\n","Before you begin the assignment, you can get a feel of how the game works by running the code in \"manual mode\" and playing a few rounds, as follows:\n","\n","```bash\n","python slot_machine.py --mode manual\n","```\n","\n","This will open a window with the slot machines, and you can click on the machines to begin accumulating reward. The game will display the number of plays on each machine and the total reward.\n","\n","After you implement the TODOs in the code, you can then run the game in \"AI mode\" to see how the $\\epsilon$-greedy algorithm performs:\n","\n","```bash\n","python slot_machine.py --mode AI\n","```"]},{"cell_type":"markdown","metadata":{"id":"NS_4xqry54WW"},"source":["Think about how the algorithm balances exploration and exploitation as you observe the game in action. You can also experiment with different values of $\\epsilon$ to see how it affects the agent's behavior. Does the AI mode perform better than the manual mode? Why or why not?"]},{"cell_type":"markdown","metadata":{"id":"GJoKenuu54WW"},"source":["## Submission\n","\n","For part 1 of the lab, you need to submit the completed notebook `Bandits.ipynb` with the code for Tasks 1-3. Make sure to run all the cells before submitting the notebook.\n","\n","Furthermore, you need to submit the completed `slot_machine.py` file with the code for Task 4.\n","\n","Note that these will be submitted along with the files for part 2 of the lab. Please see `README.md` for all the deliverables for part 1 and part 2 of the lab."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}

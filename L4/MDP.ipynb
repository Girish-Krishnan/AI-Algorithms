{"cells":[{"cell_type":"markdown","metadata":{"id":"lr77JuFCDPw0"},"source":["# COGS 188 Lab 4 Part 2: Markov Decision Processes (MDPs) and Q-Learning\n","\n","Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are widely used in automated decision-making and machine learning, particularly in the field of reinforcement learning.\n","\n","## Conceptual Overview\n","\n","An MDP is defined by:\n","- A set of states $S$\n","- A set of actions $A$\n","- A transition function $T(s, a, s')$ which is the probability of transitioning to state $s'$ from state $s$ by taking action $a$\n","- A reward function $R(s, a, s')$ which is the reward received after transitioning from state $s$ to state $s'$, due to action $a$\n","- A discount factor $\\gamma$, which represents the difference in importance between future rewards and present rewards\n","\n","The goal in an MDP is to find a policy $\\pi$, which is a function from states to actions, $\\pi: S \\rightarrow A$, that maximizes the cumulative reward.\n","\n","## Mathematical Formulation\n","\n","The value of being in a state $s$ under a policy $\\pi$, denoted $V^\\pi(s)$, is the expected return starting from $s$ and following $\\pi$ thereafter. Mathematically, this can be represented as:\n","\n","$$V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\mid S_t = s, \\pi \\right]$$\n","\n","where $R_{t+k+1}$ is the reward received at time $t+k+1$. The function $V^\\pi(s)$ is called the **state-value function** for policy $\\pi$.\n","\n","### Bellman Equation\n","\n","For a policy $\\pi$, the state-value function satisfies the Bellman equation:\n","\n","$$V^\\pi(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\sum_{s' \\in S} T(s, a, s') \\left( R(s, a, s') + \\gamma V^\\pi(s') \\right)$$\n","\n","This equation states that the value of a state under a policy is the expected return for selecting an action according to the policy, performing the action, receiving the immediate reward, and then discounted future rewards.\n","\n","## Optimal Policies and Value Functions\n","\n","The optimal state-value function $V^*(s)$ gives us the maximum value that can be obtained from state $s$ under any policy. Similarly, we define an optimal action-value function, $Q^*(s, a)$, which measures the value of taking an action $a$ in state $s$ and then following the optimal policy thereafter.\n","\n","$$Q^*(s, a) = \\sum_{s' \\in S} T(s, a, s') \\left( R(s, a, s') + \\gamma V^*(s') \\right)$$\n","\n","## How do MDPs differ from Multi-Armed Bandits?\n","\n","1. **Complexity of Environment**:\n","   - **Bandits**: No state transitions or effects of an action on future rewards.\n","   - **MDPs**: State transitions are crucial, and actions affect future states and rewards.\n","\n","2. **Scope of Decision Making**:\n","   - **Bandits**: Decisions are independent of each other.\n","   - **MDPs**: Decisions are based on the current state and have long-term consequences.\n","\n","3. **Policy Development**:\n","   - **Bandits**: The policy is simply about choosing the arm with the highest expected reward.\n","   - **MDPs**: Policies are mappings from states to actions, aiming to maximize cumulative future rewards.\n","\n","## Task 1: Implementation of a Simple Toy MDP\n","\n","In this task, you will implement and analyze different policies for a simple Markov Decision Process (MDP). This MDP represents an agent navigating a linear world with five states. Your task is to understand how different policies influence the state transitions and rewards in this MDP environment.\n","\n","### Environment Setup\n","Below is the setup for our simple MDP environment. Our environment is a linear world with 5 states, where an agent can move left, stay, or move right. We will define the states, actions, and the basic structure of our MDP functions. You can imagine this as a set of 5 blocks in a line, numbered from 1 to 5. The agent starts at block 1 and can move left, stay, or move right. If the agent tries to move left from block 1, it stays in the same block. Similarly, if the agent tries to move right from block 5, it stays in the same block."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKpUjUOoDPw7"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the states and possible actions\n","states = np.arange(1, 6)  # States 1 through 5\n","actions = ['left', 'stay', 'right']  # Actions available in each state\n","\n","print(\"States:\", states)\n","print(\"Actions:\", actions)"]},{"cell_type":"markdown","metadata":{"id":"WLsVcx3KDPw9"},"source":["Remember that each state is represented by an integer from 1 to 5, and each action is represented by a string: 'left', 'stay', or 'right'."]},{"cell_type":"markdown","metadata":{"id":"-o6EyeyjDPw9"},"source":["### Transition Function\n","Let's start by defining the transition function. This function determines the next state based on the current state and action. It should handle boundaries by preventing the agent from moving beyond the state limits. As mentioned earlier, if the agent tries to move left from state 1 or right from state 5, it stays in the same state.\n","\n","### Task: Implement the Transition Function\n","Complete the `transition` function below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"loLSfwHsDPw-"},"outputs":[],"source":["def transition(state, action):\n","    \"\"\"\n","    Transition function that determines the next state based on the current state and action.\n","\n","    Parameters:\n","    state (int): The current state.\n","    action (str): The action to be taken.\n","\n","    Returns:\n","    int: The next state.\n","    \"\"\"\n","\n","   # TODO: Implement the transition function\n","\n","# Test the transition function\n","print(\"Transition from state 1 with 'right':\", transition(1, 'right'))\n","print(\"Transition from state 5 with 'left':\", transition(5, 'left'))\n","print(\"Transition from state 1 with 'left':\", transition(1, 'left'))"]},{"cell_type":"markdown","metadata":{"id":"mRzwew93DPw-"},"source":["### Reward Function\n","Next, define the reward function, which returns a reward based on the current state and action.\n","\n","The reward function ultimately depends on what you want the agent to achieve. Let's say we want the agent to reach the rightmost state (state 5) as quickly as possible. One simple example of a reward function could be:\n","\n","- If the agent is in state 4 and moves right, it receives a reward of +10 (because it reaches the goal).\n","- In any other case, the agent receives a reward of -1.\n","\n","### Task: Implement the Reward Function\n","Complete the `reward` function as described.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2HXgzeDDPw_"},"outputs":[],"source":["def reward(state, action):\n","    \"\"\"\n","    Calculate the reward for a given state and action.\n","\n","    Parameters:\n","    state (int): The current state.\n","    action (str): The action taken.\n","\n","    Returns:\n","    int: The reward for the given state and action.\n","    \"\"\"\n","    # TODO: Implement the reward function\n","\n","# Test the reward function\n","print(\"Reward for moving right in state 4:\", reward(5, 'right'))\n","print(\"Reward for moving left in state 1:\", reward(1, 'left'))"]},{"cell_type":"markdown","metadata":{"id":"YsWu_TlLDPw_"},"source":["### Policy Implementation\n","Now, define a policy that the agent will follow. Recall that a policy is a function that maps states to actions.\n","\n","Start with a simple policy that always moves right.\n","\n","### Task: Implement the Always Right Policy\n","Fill in the `always_right_policy` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-UPNzdBDPw_"},"outputs":[],"source":["def always_right_policy(state):\n","    \"\"\"\n","    This function implements a policy that always chooses 'right'.\n","\n","    Parameters:\n","    state (int): The current state of the system.\n","\n","    Returns:\n","    str: The action chosen by the policy.\n","    \"\"\"\n","    # TODO: Implement the always right policy\n","\n","# Test the policy\n","print(\"Action chosen by always_right_policy in state 3:\", always_right_policy(3))"]},{"cell_type":"markdown","metadata":{"id":"0YqujXSzDPxA"},"source":["### Task: Implement a more random policy\n","\n","Policies can also be stochastic, meaning that they can choose actions randomly based on a certain probability distribution. Furthermore, policies can be time-dependent, meaning that the action chosen can depend on the current time step.\n","\n","Fill in the `my_policy` function. If the agent is in the leftmost state, it should move right with a probability of 0.5 and stay with a probability of 0.5. If the agent is in the rightmost state, it should move left with a probability of 0.5 and stay with a probability of 0.5. In all other states, the agent should move left with a probability of 0.3, stay with a probability of 0.3, and move right with a probability of 0.4."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNjB0MNkDPxA"},"outputs":[],"source":["def my_policy(state):\n","    \"\"\"\n","    This function implements a custom policy.\n","\n","    Parameters:\n","    state (int): The current state of the system.\n","\n","    Returns:\n","    str: The action chosen by the policy.\n","    \"\"\"\n","    # TODO: Implement a custom policy based on the description above\n"]},{"cell_type":"markdown","metadata":{"id":"tLHuYnd_DPxB"},"source":["### Simulation Function\n","Create a function to simulate the MDP given a policy. It should calculate the cumulative reward and track the number of visits to each state.\n","### Task: Implement the Simulation Function\n","Complete the `simulate_mdp` function below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsVppJBJDPxB"},"outputs":[],"source":["def simulate_mdp(policy, initial_state=1, max_steps=20):\n","    \"\"\"\n","    Simulates the Markov Decision Process (MDP) based on the given policy.\n","\n","    Parameters:\n","    - policy: A function that takes the current state as input and returns an action.\n","    - initial_state: The initial state of the MDP. Default is 1.\n","    - max_steps: The maximum number of steps to simulate. Default is 20.\n","\n","    Returns:\n","    - state_visits: An array that tracks the number of visits to each state.\n","    - cumulative_reward: The cumulative reward obtained during the simulation.\n","    - visited_history: A list that tracks the history of visited states.\n","    - reward_history: A list that tracks the history of rewards obtained.\n","    \"\"\"\n","    current_state = initial_state\n","    cumulative_reward = 0\n","    state_visits = np.zeros(len(states)) # Track the number of visits to each state\n","    visited_history = [current_state] # Track the history of visited states\n","    reward_history = [0] # Track the history of rewards\n","\n","    for _ in range(max_steps):\n","        # TODO: Implement the MDP simulation\n","        # Compute the next state, reward, and action based on the current state and policy\n","        # Update the state visits, cumulative reward, visited history, and reward history\n","        # Break (i.e. end the episode) if the current state is the last state\n","\n","\n","        pass\n","\n","    return state_visits, cumulative_reward, visited_history, reward_history\n","\n","# Simulate the MDP with the custom policy, my_policy\n","visits, total_reward, visited_history, reward_history = simulate_mdp(my_policy,max_steps=100)\n","print(\"State visits:\", visits)\n","print(\"Total reward:\", total_reward)\n","print(\"History of visited states:\", visited_history)\n","print(\"History of rewards:\", reward_history)"]},{"cell_type":"markdown","metadata":{"id":"Xds2Wto-DPxC"},"source":["### Visualization\n","\n","Firstly, we can visualize the number of visits to each state during the simulation. This will help us understand how the agent's policy affects its movement in the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OpL_x-4DPxC"},"outputs":[],"source":["def plot_state_visits(visits):\n","    plt.bar(states, visits, color='skyblue')\n","    plt.xlabel('State')\n","    plt.ylabel('Number of visits')\n","    plt.title('Number of visits to each state')\n","    plt.show()\n","\n","plot_state_visits(visits)"]},{"cell_type":"markdown","metadata":{"id":"1JyQ0VzlDPxD"},"source":["Then, we can visualize the history of states visited by the agent during the simulation and the cumulative reward obtained at each time step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sn4QZqaBDPxD"},"outputs":[],"source":["def plot_history(reward_history):\n","    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n","    ax[0].plot(reward_history, marker='o')\n","    ax[0].set_xlabel('Time step')\n","    ax[0].set_ylabel('Reward')\n","    ax[0].set_title('History of rewards')\n","    ax[1].step(np.arange(len(visited_history)), visited_history, where='mid')\n","    ax[1].set_xlabel('Time step')\n","    ax[1].set_ylabel('State')\n","    ax[1].set_yticks(states)\n","    ax[1].set_title('History of visited states')\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_history(reward_history)"]},{"cell_type":"markdown","metadata":{"id":"biHxXMg_DPxD"},"source":["Now that we've set up a basic MDP, let's take this one step further: let's implement this in 2D space for a robot navigating a grid world."]},{"cell_type":"markdown","metadata":{"id":"j2eF-26SDPxE"},"source":["## Task 2: MDPs on a Grid World using Gymnasium\n","\n","In this task, we will develop a Q-learning algorithm to solve a maze using the `gymnasium` library. We'll create a custom 2D grid environment with obstacles and learn to navigate through it using reinforcement learning.\n","\n","Firstly, we import the necessary libraries and set up the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":7,"status":"error","timestamp":1714498289733,"user":{"displayName":"Jason Fleischer","userId":"06790656365434630744"},"user_tz":420},"id":"C2IWKKZ8DPxE","outputId":"714f5e0e-702c-464a-80cf-8b8c247177c1"},"outputs":[],"source":["import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation"]},{"cell_type":"markdown","metadata":{"id":"0hsaDE8UDPxE"},"source":["Then, we create a custom maze environment using gymnasium.\n","\n","The maze environment is a 5x5 2D grid world with the following elements:\n","* The robot starts at the bottom-left corner (0, 0) and must reach the top-right corner (4, 4).\n","* There are obstacles, at the positions indicated in the code below.\n","* The robot can move in four directions: up, down, left, and right. This represents the action space, and the valid actions are represented by integers 0, 1, 2, and 3, respectively.\n","* We assume that the robot can observe everything in the environment (i.e. it knows where each obstacle is, and where the goal is located).\n","* The task is to find the shortest path from the start to the goal while avoiding obstacles.\n","\n","Read the code below to understand the setup of the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WjjjVfgDPxF"},"outputs":[],"source":["# Create a custom maze environment\n","class MazeEnv(gym.Env):\n","    metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 2}\n","\n","    def __init__(self):\n","        super(MazeEnv, self).__init__()\n","        self.size = 5  # Size of the maze (5x5)\n","        self.state = (0, 0)  # Start at bottom-left corner\n","        self.goal = (4, 4)  # Goal is at top-right corner\n","        self.obstacles = [(1, 1), (2, 1), (3, 1), (2, 4), (3, 3), (4, 1)]  # Positions of obstacles\n","        self.action_space = gym.spaces.Discrete(4)  # Up, Down, Left, Right\n","        self.observation_space = gym.spaces.Box(low=np.array([0,0]), high=np.array([4,4]), dtype=np.int32)\n","\n","    def step(self, action):\n","        # Define action effects\n","        moves = {0: (0, 1), 1: (0, -1), 2: (-1, 0), 3: (1, 0)}\n","        new_position = (self.state[0] + moves[action][0], self.state[1] + moves[action][1])\n","\n","        # Check for boundaries and obstacles\n","        if 0 <= new_position[0] < self.size and 0 <= new_position[1] < self.size and new_position not in self.obstacles:\n","            self.state = new_position\n","\n","        # Reward function\n","        if self.state == self.goal:\n","            reward = 1  # Reward for reaching the goal\n","            done = True\n","        else:\n","            reward = -0.01  # Slight negative reward for each step\n","            done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state = (0, 0)  # Reset to start position\n","        return self.state\n","\n","    def render(self, mode='human', close=False):\n","        grid = np.zeros((self.size, self.size))\n","        for obs in self.obstacles:\n","            grid[obs] = -1  # Represent obstacles\n","        grid[self.goal] = 0.5  # Represent goal\n","        grid[self.state] = 1  # Represent agent\n","        if mode == 'rgb_array':\n","            return grid\n","        elif mode == 'human':\n","            plt.imshow(grid, cmap='viridis', origin='lower')\n","            plt.grid('on')\n","            plt.show()\n","\n","# Instantiate the environment\n","env = MazeEnv()"]},{"cell_type":"markdown","metadata":{"id":"e9yLakqEDPxF"},"source":["Remember that each state is represented by a tuple of two integers $(x, y)$, which describe the coordinates of the robot. The action space is represented by integers 0, 1, 2, and 3, corresponding to moving up, down, left, and right, respectively.\n","\n","Next, we can define a **policy**, which is a function that maps states to actions. Before we try to find the best policy, let's implement a few example policies and observe how they perform in the maze environment.\n","\n","### Task: Implement the Random Policy\n","Fill in the `random_policy` function below. This policy should choose a random action at each state."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSjgUXI3DPxG"},"outputs":[],"source":["# Define a random policy for the agent\n","def random_policy(state):\n","    # TODO: Implement a random policy that chooses a random action at each step\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"RO8sfaiODPxG"},"source":["### Task: Implement a slightly more sophisticated policy\n","\n","Consider this policy:\n","\n","* If the $y$-coordinate of the robot is less than the 3, move down with a probability of 0.3 and move up with a probability of 0.7\n","* Otherwise, move right with a probability of 0.7 and move left with a probability of 0.3\n","\n","Fill in the `new_policy` function below to implement this policy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMUhai7IDPxG"},"outputs":[],"source":["# Define a more sophisticated policy for the agent\n","def new_policy(state):\n","    # TODO: Implement a policy based on the description above\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"_xAqMDvIDPxH"},"source":["### Task: Simulating the MDP\n","\n","Complete the function below to simulate the MDP for a given policy. The function should return the path taken by the robot and the total reward obtained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04GnB7jQDPxH"},"outputs":[],"source":["def simulate(env, policy, num_steps=20):\n","    \"\"\"\n","    Simulates the environment using the given policy for a specified number of steps.\n","\n","    Parameters:\n","    - env: The environment to simulate.\n","    - policy: The policy to use for selecting actions (this is a function that takes a state as input and returns an action)\n","    - num_steps: The number of steps to simulate (default: 20).\n","\n","    Returns:\n","    - path: The sequence of states visited during the simulation. (list)\n","    - total_reward: The total reward accumulated during the simulation. (float)\n","    \"\"\"\n","    state = env.reset()\n","    total_reward = 0\n","    path = [state]\n","\n","    for _ in range(num_steps):\n","        action = ... # TODO: Implement the action selection based on the policy\n","        state, reward, done, _ = env.step(action) # this will be the next state after taking the action\n","\n","        # TODO: Update the path and total_reward\n","        # If the episode is done, break\n","\n","    return path, total_reward\n","\n","# Run the simulation\n","path, total_reward = simulate(env, new_policy, num_steps=20)\n","print(\"Path taken:\", path)\n","print(\"Total reward accumulated:\", total_reward)"]},{"cell_type":"markdown","metadata":{"id":"uAHmtC0pDPxH"},"source":["Run the code below to generate a .gif file showing the path taken by the robot in the maze environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_Huua-hDPxI"},"outputs":[],"source":["def visualize_history(env, path):\n","    grid = np.zeros((env.size, env.size))\n","    for obs in env.obstacles:\n","        grid[obs] = -1  # Represent obstacles\n","    grid[env.goal] = 0.5  # Represent goal\n","\n","    fig, ax = plt.subplots()\n","\n","    def update_frame(i):\n","        ax.clear()\n","        step = path[i]\n","        grid[step] = 1\n","        if i > 0 and step != path[i-1]:\n","            grid[path[i-1]] = 0\n","        ax.imshow(grid.T, cmap='viridis', origin='lower')\n","        ax.set_title(f'Step {i+1}')\n","        ax.grid('on')\n","\n","    anim = animation.FuncAnimation(fig, update_frame, frames=len(path), interval=1000)\n","    anim.save('mdp.gif', writer='pillow')\n","\n","# Call the visualize_history function\n","visualize_history(env, path)"]},{"cell_type":"markdown","metadata":{"id":"ckDzqt2EDPxI"},"source":["The .gif can be visualized in the output cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGydfi7GDPxJ"},"outputs":[],"source":["# Display the GIF\n","from IPython.display import Image\n","Image(filename='mdp.gif')"]},{"cell_type":"markdown","metadata":{"id":"KIJ3YbAgDPxJ"},"source":["## Q-Learning\n","\n","So far, we've just tried out some simple policies in the maze environment, but we haven't yet found the optimal policy. This is where Q-learning comes in.\n","\n","Q-learning is a popular model-free reinforcement learning algorithm that is used to find the optimal action-selection policy using a Q-function. Unlike some reinforcement learning methods, Q-learning can compare the expected utility of the available actions without requiring a model of the environment. Q-learning works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. The steps involved in Q-learning are as follows:\n","\n","1. **Initialize the Q-values**: Q-values are initialized to zero (or any arbitrary value), and they represent the expected rewards for actions taken in given states.\n","\n","2. **Choose an action**: Depending on the current state, an action is selected by following an epsilon-greedy strategy, which involves choosing the best action most of the time but selecting a random action occasionally to explore the efficacy of other actions.\n","\n","3. **Perform the action and observe the reward**: After performing the action, the agent observes the reward and the new state.\n","\n","4. **Update the Q-value**: The Q-value of the previous state and action is updated using the reward received and the maximum predicted Q-value of the new state. This update is done using the Q-learning formula, which incorporates the learning rate, the discount factor, and the reward.\n","\n","5. **Repeat**: This process is repeated for each episode or until the learning is stopped.\n","\n","### Mathematical Formulation\n","\n","The Q-value update rule is as follows:\n","$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n","where:\n","- $Q(s, a)$ is the current Q-value of state $s$ and action $a$.\n","- $\\alpha$ is the learning rate, $\\alpha > 0$.\n","- $R(s,a)$ is the reward received after taking action $a$ in state $s$.\n","- $\\gamma$ is the discount factor $0 \\leq  \\gamma  \\leq 1$, which determines the importance of future rewards.\n","- $\\max_{a'} Q(s', a')$ is the maximum predicted Q-value in the new state $s'$, over all possible actions $a'$.\n","\n","This update rule helps the agent learn the expected reward for taking an action in a given state, and it converges to the optimal action-value function as the agent continues to learn.\n","\n","Now, let's implement the Q-learning algorithm. Since the state space and action space are discrete, we can represent the Q-values as a table, where for every state-action pair, we have a corresponding Q-value.\n","\n","### Task: Implement the Q-Learning Algorithm\n","\n","Fill in the `q_learning` function below to implement the Q-learning algorithm. The function should return the Q-table learned by the agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvELhWjMDPxK"},"outputs":[],"source":["def q_learning(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n","    # TODO: initialize the Q-table with zeros using np.zeros\n","    # Think about what the dimensions of the Q-table should be. Remember that the grid is 5x5 and there are 4 possible actions.\n","    q_table = ...\n","\n","    for episode in range(episodes):\n","        state = env.reset() # Start a new episode\n","        done = False\n","\n","        while not done:\n","            # TODO: Choose an action using an epsilon-greedy policy\n","\n","            # TODO: use env.step() to find the next state and reward based on the chosen action\n","\n","            # TODO: Update the Q-table using the Q-learning update rule\n","\n","            # TODO: Update the current state\n","\n","            pass\n","\n","    return q_table\n","\n","# Train the Q-learning model\n","q_table = q_learning(env)"]},{"cell_type":"markdown","metadata":{"id":"GTd71WIZDPxM"},"source":["We can then visualize the optimal policy for each state in the maze environment. This is done by selecting the action with the highest Q-value at each state.\n","\n","Recall that:\n","* 0 represents moving up\n","* 1 represents moving down\n","* 2 represents moving left\n","* 3 represents moving right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVCduVcxDPxM"},"outputs":[],"source":["# Visualize the optimal policy from the Q-table\n","def visualize_policy(q_table):\n","    policy = np.argmax(q_table, axis=2)\n","    plt.imshow(policy.T, origin='lower')\n","    plt.colorbar()\n","    plt.title('Optimal Policy')\n","    plt.show()\n","\n","visualize_policy(q_table)"]},{"cell_type":"markdown","metadata":{"id":"9jaV8KRSDPxN"},"source":["## Visualize Maze Navigation\n","\n","Finally, let's simulate the agent navigating the maze with the learned Q-table and visualize the process. This code will generate a .gif file, which will be saved in `mdp_q_learning.gif`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9CH2FsfDPxN"},"outputs":[],"source":["import matplotlib.animation as animation\n","\n","def simulate(env, q_table):\n","    state = env.reset()\n","    done = False\n","\n","    starting_frame = env.render(mode='rgb_array').T\n","    frames = [starting_frame]  # List to store frames for animation\n","\n","    while not done:\n","        # Now that we have the Q-table, we can choose the best action based on the Q-values\n","        action = np.argmax(q_table[state[0], state[1]])\n","        state, _, done, _ = env.step(action)\n","        frame = env.render(mode='rgb_array').T  # Render the environment as an RGB array\n","        frames.append(frame)\n","\n","    def update_frame(i):\n","        ax.clear()\n","        ax.imshow(frames[i], cmap='viridis', origin='lower')\n","        ax.set_title(f'Step {i+1}')\n","        ax.grid('on')\n","\n","    # Create animation from frames\n","    fig, ax = plt.subplots()\n","    anim = animation.FuncAnimation(fig, update_frame, frames=len(frames), interval=500)\n","    anim.save('mdp_q_learning.gif', writer='pillow')\n","\n","# Call the simulate function\n","simulate(env, q_table)"]},{"cell_type":"markdown","metadata":{"id":"Gxqg624lDPxO"},"source":["The resulting .gif file can be visualized in the output cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-k3cadgDPxO"},"outputs":[],"source":["# Display the GIF\n","Image(filename='mdp_q_learning.gif')"]},{"cell_type":"markdown","metadata":{"id":"ZQNNOIfnDPxP"},"source":["To verify that you implemented the Q-learning algorithm correctly, you can check that the robot reaches the goal following an efficient trajectory that avoids obstacles."]},{"cell_type":"markdown","metadata":{"id":"iXaCnzoXDPxP"},"source":["## Submission\n","\n","Please submit your completed notebook `MDP.ipynb` to Gradescope, along with the gifs `mdp.gif` and `mdp_q_learning.gif` generated in the last two sections. This should be turned in along with `Bandits.ipynb` and `slot_machine.py` from Lab 4 Part 1.\n","\n","During Lab 5, we will explore dynamic programming, a method that can be used to solve the Bellman equation and find optimal policies for MDPs."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COGS 188 Spring 2024: Lab 1\n",
        "\n",
        "**Due Date: April 15th, 11:59 PM**\n",
        "\n",
        "In this programming assignment, we will explore how to navigate the web by finding a path from one webpage to another. This involves understanding the web as a directed graph, where web pages are nodes connected by directed edges representing hyperlinks from one page to another.\n",
        "\n",
        "We will use three search strategies:\n",
        "\n",
        "1. **Breadth-First Search (BFS)**: A layer-by-layer traversal method.\n",
        "2. **Depth-First Search (DFS)**: An exploration of a node's branches before its neighbors.\n",
        "3. **Bidirectional Search**: A simultaneous search from both the start and target nodes.\n",
        "\n",
        "Additionally, we will learn how to use Python libraries like `requests`, `BeautifulSoup`, and `googlesearch-python` to interact with web content and perform searches.\n",
        "\n",
        "## Submission Instructions\n",
        "\n",
        "After you finish this assignment, export this Jupyter notebook as a **.py** file and upload the resulting Python script to Gradescope.\n",
        "\n",
        "## Imports\n",
        "\n",
        "### `requests`\n",
        "\n",
        "The `requests` library is one of the most popular Python libraries for making HTTP requests. It simplifies the process of sending HTTP requests to web servers and handling responses. We use `requests` to fetch the content of web pages.\n",
        "\n",
        "### `BeautifulSoup`\n",
        "\n",
        "`BeautifulSoup` is a library for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data easily. We use `BeautifulSoup` to parse the HTML content of web pages fetched with `requests` and to extract hyperlinks.\n",
        "\n",
        "### `collections.deque`\n",
        "\n",
        "The `deque` (double-ended queue) from the `collections` module is an enhanced list-like container with faster appends and pops from both ends. It's ideal for queues and breadth-first search implementations where elements are frequently added and removed.\n",
        "\n",
        "This [guide](https://www.geeksforgeeks.org/deque-in-python/) provides some helpful information on how to work with double-ended queues.\n",
        "\n",
        "### `urllib.parse`\n",
        "\n",
        "The `urllib.parse` module provides a standard interface for breaking Uniform Resource Locator (URL) strings up in components (addressing scheme, network location, path, etc.), to combine the components back into a URL string, and to convert a “relative URL” to an absolute URL given a “base URL.” We use `urljoin` from this module to resolve relative URLs to absolute URLs, ensuring we always work with complete URLs."
      ],
      "metadata": {
        "id": "hVg817R3apIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you haven't installed requests and beautifulsoup4, you can uncomment the following:\n",
        "\n",
        "# !pip install requests beautifulsoup4"
      ],
      "metadata": {
        "id": "0cqImj3tKAk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvCJGnr1alOo"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import deque\n",
        "from urllib.parse import urljoin, urlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching Links from a Webpage\n",
        "\n",
        "To navigate the web, we first need to understand how to extract hyperlinks from a webpage. We will use the `requests` library to fetch the webpage content and `BeautifulSoup` to parse this content and extract links.\n"
      ],
      "metadata": {
        "id": "46yBvfvJb7iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links(url):\n",
        "    \"\"\"Fetches links from the given URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = set()\n",
        "\n",
        "        for link in soup.find_all('a', href=True): # the <a> tag in HTML indicates a link\n",
        "            absolute_link = urljoin(url, link['href'])\n",
        "            if urlparse(absolute_link).scheme in ['http', 'https']:\n",
        "                links.add(absolute_link)\n",
        "\n",
        "        return links\n",
        "    except requests.RequestException:\n",
        "        return set()"
      ],
      "metadata": {
        "id": "KcmYVGLtb9J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the `get_links()` function returns a **set**  of links leading from a given webpage, and each link is a **string**."
      ],
      "metadata": {
        "id": "saKIi-MmqMil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_links(\"https://cogsci.ucsd.edu/\")"
      ],
      "metadata": {
        "id": "vSQrVww0luGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Breadth-First Search (BFS)\n",
        "\n",
        "BFS is a fundamental algorithm for traversing or searching tree or graph data structures. It starts at a selected node and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n",
        "\n",
        "Your task is to complete the function below to implement BFS to find the shortest path between two webpages."
      ],
      "metadata": {
        "id": "Z9EJoJgKcAm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bfs(start_url, target_url):\n",
        "    \"\"\"Performs BFS to find the shortest path from start_url to target_url.\"\"\"\n",
        "    # visited is the set of URLs that have been visited so far\n",
        "    visited = set([start_url])\n",
        "\n",
        "    # Create a queue of tuples (URL, path), initialized with the starting URL\n",
        "    queue = deque([(start_url, [start_url])])\n",
        "\n",
        "    while queue:\n",
        "        current_url, path = ... # Fill in this blank to remove an element from the left of the queue\n",
        "        print(f\"Visiting {current_url}\")\n",
        "\n",
        "        if current_url == target_url:\n",
        "            ... # Fill in this blank to indicate what happens when the target URL is reached successfully\n",
        "\n",
        "        for link in get_links(current_url):\n",
        "            if link not in visited:\n",
        "                # Fill in the blank below to add the link to the visited set\n",
        "                ...\n",
        "\n",
        "                queue.append((link, path + [link])) # This adds the link and the path to that link to the right end of the queue\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "p2MYzpPOcDXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running BFS\n",
        "\n",
        "Let's run BFS using two example webpages to find a path of hyperlinks connecting them. For this, we define a function called `find_path_bfs` that runs the BFS function described above and prints out the resulting path."
      ],
      "metadata": {
        "id": "5xhvaH2MfySd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_path_bfs(start_url, target_url):\n",
        "    \"\"\"Finds a path of links from start_url to target_url.\"\"\"\n",
        "    path = bfs(start_url, target_url)\n",
        "\n",
        "    if path:\n",
        "        print(\"Path found:\")\n",
        "        for url in path:\n",
        "            print(url)\n",
        "    else:\n",
        "        print(\"No path found.\")"
      ],
      "metadata": {
        "id": "qNzCnoKEgBnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: You can expect this cell to take at least 15 seconds to run."
      ],
      "metadata": {
        "id": "n6gG997MouQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_url = 'https://books.toscrape.com/index.html'\n",
        "target_url = 'https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html'\n",
        "find_path_bfs(start_url, target_url)"
      ],
      "metadata": {
        "id": "Qv2GIpHkgQy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we try using another approach: depth-first search?"
      ],
      "metadata": {
        "id": "yD25OamqgVjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Depth-First Search (DFS)\n",
        "\n",
        "DFS is another fundamental algorithm that uses a different strategy than BFS. It explores as far as possible along each branch before backtracking. This means it goes deep into the graph as quickly as possible.\n",
        "\n",
        "Now, your next task is to apply DFS to our web path finding problem."
      ],
      "metadata": {
        "id": "9dhQHip3cGAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dfs(start_url, target_url, visited=None, path=None):\n",
        "    \"\"\"Performs DFS to find a path from start_url to target_url.\"\"\"\n",
        "    if visited is None:\n",
        "        visited = set() # Define a set of visited nodes\n",
        "    if path is None:\n",
        "        path = [start_url]\n",
        "\n",
        "    # Fill in the blank below to add the start url to the set of visited nodes.\n",
        "    ...\n",
        "\n",
        "    print(f\"Visiting {start_url}\")\n",
        "\n",
        "    if start_url == target_url:\n",
        "        print(\"\\nTarget found.\")\n",
        "        # Fill in the blank to indicate what happens when the target is found\n",
        "        ...\n",
        "\n",
        "    for link in get_links(start_url):\n",
        "        if link not in visited:\n",
        "\n",
        "            # This recursively applies DFS to the current link\n",
        "            result_path = dfs(link, target_url, visited, path + [link])\n",
        "\n",
        "            if result_path is not None:\n",
        "                return result_path\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "lVg0fAoacIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running DFS\n",
        "\n",
        "Similar to before, let's try running DFS using the same two webpages provided earlier."
      ],
      "metadata": {
        "id": "GY8BN7hLgTyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_path_dfs(start_url, target_url):\n",
        "    \"\"\"Finds a path of links from start_url to target_url using DFS.\"\"\"\n",
        "    path = dfs(start_url, target_url)\n",
        "\n",
        "    if path:\n",
        "        print(\"\\nPath found:\")\n",
        "        for url in path:\n",
        "            print(url)\n",
        "    else:\n",
        "        print(\"\\nNo path found.\")"
      ],
      "metadata": {
        "id": "l9inSmNZgr5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to see the algorithm in action.\n",
        "\n",
        "**NOTE**: If this cell takes more than 5 minutes to run, feel free to press the **STOP** button to interrupt execution."
      ],
      "metadata": {
        "id": "EHEzcS1Dgz37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_url = 'https://books.toscrape.com/index.html'\n",
        "target_url = 'https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html'\n",
        "find_path_dfs(start_url, target_url)"
      ],
      "metadata": {
        "id": "cjPqOBqUgwPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional, ungraded question:** You might notice that DFS seems to take a really long time to run. Why do you think this is the case?"
      ],
      "metadata": {
        "id": "wxH6YAz8hHFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional Search\n",
        "\n",
        "Bidirectional Search is an advanced search technique that runs two simultaneous searches—one forward from the start node, and the other backward from the target node. The search stops when the two meet in the middle.\n",
        "\n",
        "To implement the backward search, we will use a function to find incoming links to a page. This is where `googlesearch-python` comes into play. The `googlesearch-python` library is a third-party tool that allows us to use Google search within our Python scripts. It's particularly useful for finding incoming links to a webpage, a task that is otherwise quite complex to automate. We utilize this library for the bidirectional search to simulate searching for pages linking to our target URL."
      ],
      "metadata": {
        "id": "z5BZDDzncNeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googlesearch-python"
      ],
      "metadata": {
        "id": "58qoST4jcOqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caution: Usage Limits of `googlesearch-python`\n",
        "\n",
        "When using the `googlesearch-python` library to programmatically perform Google searches, it's important to be mindful of Google's usage policies and limitations. Automated queries can quickly reach the rate limits imposed by Google, potentially leading to your IP being temporarily blocked from making further requests.\n",
        "\n",
        "Remember, the goal of tools like `googlesearch-python` is to facilitate learning and small-scale automation. They are not intended for large-scale data extraction or activities that could harm the availability and reliability of web services.\n",
        "\n",
        "### Helpful Tip\n",
        "\n",
        "If you run into API rate limit issues, I would recommend deleting the current runtime and reconnecting to the server. If you work on Google Colab, you can disconnect and delete the runtime, and then reconnect. If you end up doing this, you'll have to re-run the first cell (that imports all the libraries)."
      ],
      "metadata": {
        "id": "R3ojGPsQkY-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Incoming Links\n",
        "\n",
        "Finding incoming links (or backlinks) to a webpage is challenging because webpages don't inherently list pages that link to them. However, we can use Google search to find such links by searching for pages that link to our target URL.\n",
        "\n",
        "We will define a function `get_incoming_links` using `googlesearch-python` to perform this task."
      ],
      "metadata": {
        "id": "m0LPEv27cTGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googlesearch import search\n",
        "\n",
        "def get_incoming_links(url, num_results=10):\n",
        "    \"\"\"Finds pages that link to the specified URL using Google search.\"\"\"\n",
        "    query = f\"link:{url}\"\n",
        "    # For simplicity, we limit our search to num_results\n",
        "    for result in search(query, num_results=num_results, sleep_interval=5): # the sleep_interval ensures that requests aren't sent too quickly\n",
        "        yield result"
      ],
      "metadata": {
        "id": "ob8k1CI9cRDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Implementing Bidirectional Search\n",
        "\n",
        "With the ability to find both direct links and incoming links, we can now implement the bidirectional search algorithm. This algorithm alternates between expanding the forward frontier and the backward frontier until a connection is found.\n",
        "\n",
        "Your task is to fill in the blanks below to complete the implementation of bidirectional search."
      ],
      "metadata": {
        "id": "EHWzHd4kcYzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bidirectional_search(start_url, target_url):\n",
        "    forward_queue = deque([(start_url, [start_url])])\n",
        "    backward_queue = deque([(target_url, [target_url])])\n",
        "    forward_visited = {start_url}\n",
        "    backward_visited = {target_url}\n",
        "    forward_paths = {start_url: [start_url]}\n",
        "    backward_paths = {target_url: [target_url]}\n",
        "\n",
        "    while forward_queue and backward_queue:\n",
        "        # Forward search step\n",
        "        # Fill in the blank below to remove an element from the left of the queue\n",
        "        current_forward, path_forward = ...\n",
        "        print(f\"Forward visiting: {current_forward}\")\n",
        "        for link in ...: # Fill in the blank here to iterate over all links from a webpage\n",
        "            if link not in ...: # Fill in this blank\n",
        "                forward_visited.add(link)\n",
        "                new_path = path_forward + [link]\n",
        "                forward_paths[link] = new_path\n",
        "                forward_queue.append((link, new_path))\n",
        "                if link in backward_visited:\n",
        "                    return forward_paths[link] + backward_paths[link][::-1][1:]\n",
        "\n",
        "        # Write some code below to implement the backward search step\n",
        "        # It follows a very similar structure to the forward search step, but we iterate over all INCOMING links instead\n",
        "\n",
        "        # INSERT YOUR CODE HERE\n",
        "\n",
        "    return None  # If no connection is found"
      ],
      "metadata": {
        "id": "ArOgivAScZlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Bidirectional Search\n",
        "\n",
        "Similar to before, let's try running bidirectional search on the same two webpages we used earlier.\n",
        "\n",
        "Run the cell below."
      ],
      "metadata": {
        "id": "Rmt9TEbZhajo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_path_bidirectional(start_url, target_url):\n",
        "    \"\"\"Finds a path of links from start_url to target_url using bidirectional search.\"\"\"\n",
        "    path = bidirectional_search(start_url, target_url)\n",
        "\n",
        "    if path:\n",
        "        print(\"Path found:\")\n",
        "        for url in path:\n",
        "            print(url)\n",
        "    else:\n",
        "        print(\"No path found.\")"
      ],
      "metadata": {
        "id": "-6e6GJokhf-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_url = 'https://books.toscrape.com/index.html'\n",
        "target_url = 'https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html'\n",
        "find_path_bidirectional(start_url, target_url)"
      ],
      "metadata": {
        "id": "2lw8QBgyhigV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional, ungraded question:** Comment on your observations after you run bidirectional search. Does it run faster or slower compared to BFS and DFS? Why do you think this is the case?"
      ],
      "metadata": {
        "id": "wPLjtQUDhmhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission and Grading\n",
        "\n",
        "Check that you've correctly implemented the `bfs`, `dfs`, and `bidirectional_search` functions above, by filling in the blanks. These functions will be graded.\n",
        "\n",
        "Once you're done with the assignment, **export this notebook as a .py file**, and turn in the .py file to **Gradescope**."
      ],
      "metadata": {
        "id": "GKIOPyGbpgMo"
      }
    }
  ]
}